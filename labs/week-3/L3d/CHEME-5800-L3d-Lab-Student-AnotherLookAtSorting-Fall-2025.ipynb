{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa33aa4-2de3-4223-b150-8b561d7a99c5",
   "metadata": {},
   "source": [
    "# L3d: The Quicksort Algorithm\n",
    "In this lab, we compare the average runtime of a recursive [Quicksort](https://en.wikipedia.org/wiki/Quicksort) implementation to a [Bubblesort implementation](https://en.wikipedia.org/wiki/Bubble_sort) and [Julia's built-in sort function](https://docs.julialang.org/en/v1/base/sort/#Base.sort) using [the BenchmarkTools.jl package](https://github.com/JuliaCI/BenchmarkTools.jl). \n",
    "\n",
    "We'll complete three main tasks in this notebook:\n",
    "- **Task 1**: In this task, you will implement the Bubblesort algorithm, test your implementation for correctness, and benchmark its performance across different array sizes to establish a baseline to compare other algorithms against.\n",
    "- **Task 2**: In the second task, we'll test an existing Quicksort implementation for correctness and benchmark its performance to compare against Bubblesort.\n",
    "- **Task 3**: Finally, we'll benchmark Julia's built-in sort function to see how our custom implementations compare against an optimized library solution.\n",
    "\n",
    "### Algorithms\n",
    "Bubblesort is a simple method of ordering a list. \n",
    "\n",
    "> __How does Bubblesort work?__ It involves repeatedly passing through the list, comparing adjacent items, and swapping any two neighboring items that are out of order. The algorithm gets its name because smaller elements \"bubble\" to the top of the list, just like how air bubbles rise to the surface of water.\n",
    "> \n",
    "> Let's take a look at the Bubblesort algorithm in more detail [here](CHEME-5800-L3d-Algorithm-Bubblesort-Fall-2025.ipynb).\n",
    "\n",
    "\n",
    "Quicksort takes a different approach to sorting. \n",
    "\n",
    "> __How does Quicksort work?__ Quicksort is a recursive sorting algorithm that works by selecting a pivot element and partitioning the remaining elements into two sub-arrays based on their value relative to the pivot. The algorithm then recursively sorts the sub-arrays until they have fewer than two elements. The choice of pivot is critical for the algorithm's efficiency.\n",
    "> \n",
    "> Let's take a look at the Quicksort algorithm [here](CHEME-5800-L3d-Algorithm-Quicksort-Fall-2025.ipynb).\n",
    "\n",
    "We are going to see some surprising results! Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a44b1c-5b62-4fe6-9d4b-0fc891d20505",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [include command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a22ed4-eb93-4c07-996c-4da0b2cfe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-student.jl\")); # what is this doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b4aa",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ad70a-c6c2-44f7-bb40-cbd78e471ea1",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Before we get started, let's set up some constants. See the comment next to each value for what it is, its permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36bca24-c120-4ac2-88b0-bffd5a522688",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_trials = 12; # exponent\n",
    "number_of_items_per_trial = [2^i for i ∈ 1:max_number_of_trials]; # this is an array comprehension, yet another iteration pattern!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945cf4b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842921c-5de9-4de0-9f90-ceb1b2562315",
   "metadata": {},
   "source": [
    "## Task 1: Establish a performance baseline: Bubblesort\n",
    "In this task, we show that our bubblesort implementation works as expected. We also establish a performance baseline for comparison with the Quicksort algorithm.\n",
    "\n",
    "> We'll use [the `Test.jl` package](https://docs.julialang.org/en/v1/stdlib/Test/) to write __unit tests__ for our bubble sort implementation. The [`Test.jl` package](https://docs.julialang.org/en/v1/stdlib/Test/) provides a framework for writing and running tests in Julia, including support for assertions, test cases, and test suites. Let's use [the @test macro](https://docs.julialang.org/en/v1/stdlib/Test/#Test.@test) to check that our bubble sort implementation works correctly.\n",
    "\n",
    "So, did we pass the tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "let|\n",
    "\n",
    "    # initialize -\n",
    "    N = 1000; # number of elements in the random vector\n",
    "    arr = rand(N); # random vector length N\n",
    "\n",
    "    # check: do we get the same result as the built-in sort function?\n",
    "    @test sort(arr) == bubblesort(arr) # if the test fails, an error is thrown!\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d7073-f7a0-4af9-87e3-0aa3e09085eb",
   "metadata": {},
   "source": [
    "Okay, so if we get here, all seems to be good with [our `bubblesort(...)` implementation](src/Compute.jl), so let's see how our code performs as we increase the size of the vector that we are sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd4428",
   "metadata": {},
   "source": [
    "Now let's measure how [our `bubblesort(...)` function](src/Compute.jl) performs as we increase the size of the input array. We'll use the [BenchmarkTools.jl package](https://github.com/JuliaCI/BenchmarkTools.jl) to get accurate timing measurements.\n",
    "\n",
    "The code below uses a [`let ... end` block](https://docs.julialang.org/en/v1/manual/variables-and-scoping/#Let-Blocks) to create a local scope and performs the following steps:\n",
    "1. **Initialize a DataFrame** to store our results with columns for array size (`n`), mean runtime (`μ`), and standard deviation (`σ`) using the [DataFrames.jl package](https://github.com/JuliaData/DataFrames.jl)\n",
    "2. **Loop through different array sizes** from our `number_of_items_per_trial` array (powers of 2 from $2^{1}$ to $2^{12}$)\n",
    "3. **For each array size:**\n",
    "   - Create a benchmark using `@benchmarkable` that will test our `bubblesort` function\n",
    "   - Use `setup=` to generate fresh random data for each trial (avoiding measuring data generation time)\n",
    "   - Run the benchmark multiple times and collect timing statistics\n",
    "   - Store the results (array size, mean time, standard deviation) as a row in our DataFrame\n",
    "\n",
    "The result will be stored in the `bubble_sort_data::DataFrame` variable containing performance data that we can analyze and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f29e5f-9c1a-4f19-9ef4-d16df844a0b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `swapped` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `swapped` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      "  [1] \u001b[0m\u001b[1mbubblesort\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90marray\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Documents/GitHub/CHEME-5800-Labs-Fall-2025/labs/week-3/L3d/src/\u001b[39m\u001b[90m\u001b[4mCompute.jl:26\u001b[24m\u001b[39m",
      "  [2] \u001b[0m\u001b[1mvar\"##core#231\"\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msize_of_rand_vec_to_sort#230\u001b[39m::\u001b[0mInt64, \u001b[90mdata\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:598\u001b[24m\u001b[39m",
      "  [3] \u001b[0m\u001b[1mvar\"##sample#232\"\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mTuple\u001b[90m{Int64}\u001b[39m, \u001b[90m__params\u001b[39m::\u001b[0mBenchmarkTools.Parameters\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:607\u001b[24m\u001b[39m",
      "  [4] \u001b[0m\u001b[1m_lineartrial\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mb\u001b[39m::\u001b[0mBenchmarkTools.Benchmark, \u001b[90mp\u001b[39m::\u001b[0mBenchmarkTools.Parameters; \u001b[90mmaxevals\u001b[39m::\u001b[0mInt64, \u001b[90mkwargs\u001b[39m::\u001b[0m@Kwargs\u001b[90m{}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[32mBenchmarkTools\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:186\u001b[24m\u001b[39m",
      "  [5] \u001b[0m\u001b[1m_lineartrial\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mb\u001b[39m::\u001b[0mBenchmarkTools.Benchmark, \u001b[90mp\u001b[39m::\u001b[0mBenchmarkTools.Parameters\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[32mBenchmarkTools\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:181\u001b[24m\u001b[39m",
      "  [6] \u001b[0m\u001b[1m#invokelatest#2\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:1055\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      "  [7] \u001b[0m\u001b[1minvokelatest\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:1052\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      "  [8] \u001b[0m\u001b[1m#lineartrial#46\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:51\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      "  [9] \u001b[0m\u001b[1mlineartrial\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:50\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      " [10] \u001b[0m\u001b[1mtune!\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mb\u001b[39m::\u001b[0mBenchmarkTools.Benchmark, \u001b[90mp\u001b[39m::\u001b[0mBenchmarkTools.Parameters; \u001b[90mprogressid\u001b[39m::\u001b[0mNothing, \u001b[90mnleaves\u001b[39m::\u001b[0mFloat64, \u001b[90mndone\u001b[39m::\u001b[0mFloat64, \u001b[90mverbose\u001b[39m::\u001b[0mBool, \u001b[90mpad\u001b[39m::\u001b[0mString, \u001b[90mkwargs\u001b[39m::\u001b[0m@Kwargs\u001b[90m{}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[32mBenchmarkTools\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:299\u001b[24m\u001b[39m",
      " [11] \u001b[0m\u001b[1mtune!\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:288\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      " [12] \u001b[0m\u001b[1mtune!\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mb\u001b[39m::\u001b[0mBenchmarkTools.Benchmark\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m    @\u001b[39m \u001b[32mBenchmarkTools\u001b[39m \u001b[90m~/.julia/packages/BenchmarkTools/1i1mY/src/\u001b[39m\u001b[90m\u001b[4mexecution.jl:288\u001b[24m\u001b[39m",
      " [13] top-level scope",
      "\u001b[90m    @\u001b[39m \u001b[90m\u001b[4mIn[4]:8\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "bubble_sort_data = let\n",
    "    bubble_sort_data = DataFrame();\n",
    "    for i ∈ eachindex(number_of_items_per_trial)\n",
    "        size_of_rand_vec_to_sort = number_of_items_per_trial[i];\n",
    "    \n",
    "        # run the test with different size vectors -\n",
    "        test_run = @benchmarkable bubblesort(data) setup=(data=rand(0:number_of_items_per_trial[end], $(size_of_rand_vec_to_sort)));\n",
    "        tune!(test_run)\n",
    "        results = run(test_run)\n",
    "    \n",
    "        # store the results -\n",
    "        row = (\n",
    "            n = size_of_rand_vec_to_sort,\n",
    "            μ = mean(results.times),\n",
    "            σ = std(results.times)\n",
    "        );\n",
    "        push!(bubble_sort_data, row)\n",
    "    end\n",
    "    bubble_sort_data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdaab6-5cb6-47af-9647-61f0c93c3eaf",
   "metadata": {},
   "source": [
    "## Task 2: Quicksort\n",
    "In this task, we verify that our Quicksort implementation works as expected. We also establish the performance of this method in comparison with the Bubblesort algorithm.\n",
    "\n",
    "> As before, we'll use [the `Test.jl` package](https://docs.julialang.org/en/v1/stdlib/Test/) to write a __unit test__ for [our `quicksort(...)` implementation](src/Compute.jl). The [`Test.jl` package](https://docs.julialang.org/en/v1/stdlib/Test/) provides a framework for writing and running tests in Julia, including support for assertions, test cases, and test suites. Let's use [the @test macro](https://docs.julialang.org/en/v1/stdlib/Test/#Test.@test) to check that [our `quicksort(...)` implementation](src/Compute.jl) works correctly.\n",
    "\n",
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417c1c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    N = 1000; # number of elements in the random vector\n",
    "    arr = rand(N); # random vector length N\n",
    "\n",
    "    # check: do we get the same result as the built-in sort function?\n",
    "    @test sort(arr) == quicksort(arr) # if the test fails, an error is thrown!\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bc575-bf70-4d9d-a0c5-65ec3d723400",
   "metadata": {},
   "source": [
    "Okay, so if we get here, all seems to be good with our `quicksort(...)` implementation, so let's see how our code performs as we increase the vector size to be sorted. \n",
    "\n",
    "Let's use the same benchmarking pattern as we did for the Bubblesort algorithm. The benchmarking results for the Quicksort algorithm will be stored in the `quick_sort_data::DataFrame` variable containing performance data that we can analyze and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c4780-d8e8-473e-b6dc-b1ef43374b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_sort_data = let\n",
    "    \n",
    "    quick_sort_data = DataFrame();\n",
    "    for i ∈ eachindex(number_of_items_per_trial)\n",
    "        size_of_rand_vec_to_sort = number_of_items_per_trial[i];\n",
    "    \n",
    "        # run the test with different size vectors -\n",
    "        test_run = @benchmarkable quicksort(data) setup=(data=rand(0:number_of_items_per_trial[end], $(size_of_rand_vec_to_sort)));\n",
    "        tune!(test_run)\n",
    "        results = run(test_run)\n",
    "    \n",
    "        # store the results -\n",
    "        row = (\n",
    "            n = size_of_rand_vec_to_sort,\n",
    "            μ = mean(results.times),\n",
    "            σ = std(results.times)\n",
    "        );\n",
    "        push!(quick_sort_data, row)\n",
    "    end\n",
    "    quick_sort_data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3a53f-a766-489b-9147-67d82df6c2a3",
   "metadata": {},
   "source": [
    "## Task 3: What is the scaling of the Built-in sort function?\n",
    "Julia provides [sophisticated multi-method sorting capability](https://docs.julialang.org/en/v1/base/sort/#Sorting-Functions). How do our implementations perform against what Julia can offer? \n",
    "\n",
    "> __Buy versus build:__ This is yet another example of the __buy versus build__ conundrum. Should we build our own implementation, or __buy__ someone else's? You should (almost) always __buy__, and benefit from the hard (optimized) work of others. But let's see if that is true in this case.\n",
    "\n",
    "We'll use the same benchmarking approach as we did for the Bubblesort and Quicksort algorithms. The results will be stored in the `julia_sort_data::DataFrame` variable containing performance data that we can analyze and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be476bf-8dac-47c3-b1b3-263a26b58faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_sort_data = let\n",
    "    julia_sort_data = DataFrame();\n",
    "    for i ∈ eachindex(number_of_items_per_trial)\n",
    "        size_of_rand_vec_to_sort = number_of_items_per_trial[i];\n",
    "    \n",
    "        # run the test with different size vectors -\n",
    "        test_run = @benchmarkable sort(data) setup=(data=rand(0:number_of_items_per_trial[end], $(size_of_rand_vec_to_sort)));\n",
    "        tune!(test_run)\n",
    "        results = run(test_run)\n",
    "    \n",
    "        # store the results -\n",
    "        row = (\n",
    "            n = size_of_rand_vec_to_sort,\n",
    "            μ = mean(results.times),\n",
    "            σ = std(results.times)\n",
    "        );\n",
    "        push!(julia_sort_data, row)\n",
    "    end\n",
    "    julia_sort_data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6424a8b-3cf4-487a-9fdc-257effb28e14",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "`Unhide` the code below to see how we plotted the average runtime of each sorting method as a function of the length of the vector $n$.\n",
    "\n",
    "> __Summary__: For very short sequences, our `bubblesort(...)` implementation is the winner! However, once the sequences become large, [Julia `sort(...)` implementations are the clear winners](https://docs.julialang.org/en/v1/base/sort/#Sorting-Functions). Something interesting here: our `quicksort(...)` implementation seems to have similar scaling behavior to [the built-in `sort(...)` method](https://docs.julialang.org/en/v1/base/sort/#Sorting-Functions).\n",
    "\n",
    "These results show the power of constants! When we do scaling analysis, we ignore constants, but they are important in practice. For example, Quicksort and Julia's built-in sort function have similar scaling behavior, but the constants are different (the built-in sort implementation appears to have a smaller constant factor, meaning it is faster in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21de068-ea54-47b9-9bab-c562b781aeb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    plot(quick_sort_data[:,:n], quick_sort_data[:,:μ], label=\"quicksort\", \n",
    "        yscale=:log10, xscale=:log10, lw=3, c=:gray69, minorgrid=true, legend=:topleft)\n",
    "    plot!(bubble_sort_data[:,:n], bubble_sort_data[:,:μ], label=\"bubblesort\", \n",
    "        yscale=:log10, xscale=:log10, lw=3, c=:red)\n",
    "    plot!(julia_sort_data[:,:n], julia_sort_data[:,:μ], label=\"Julia sort\", \n",
    "        yscale=:log10, xscale=:log10, lw=3, c=:blue)\n",
    "    xlims!(1e+0, 1e+4)\n",
    "    ylims!(1e+0, 1e+7)\n",
    "    xlabel!(\"Number of elements n\", fontsize=18)\n",
    "    ylabel!(\"Mean Runtime (ns)\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c5608",
   "metadata": {},
   "source": [
    "## Summary\n",
    "So what did we learn from comparing these three sorting approaches? Surprisingly, our simple bubblesort actually wins for very small datasets, but it quickly becomes the slowest as the data grows due to its O(n²) complexity.\n",
    "\n",
    "Our quicksort implementation shows much better scaling behavior, performing similarly to Julia's built-in sort in terms of growth rate. However, Julia's optimized sort function consistently outperforms our implementation with better constants and lower overhead.\n",
    "\n",
    "The key takeaway? This is a perfect example of the \"buy vs build\" principle in programming. While implementing algorithms yourself is great for learning, production code should almost always use well-tested library functions that have been optimized by experts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56872f86-f8cf-4019-994a-25c7550c17e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8409c-0489-4bc4-8f3c-c61e65c08153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
